{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ETL ‚Äî Accidents de la route\n",
    "\n",
    "Ce notebook montre, pas √† pas, comment :\n",
    "1) se connecter √† l‚ÄôAPI publique Opendatasoft,\n",
    "2) r√©cup√©rer un petit √©chantillon,\n",
    "3) paginer pour extraire un volume plus grand,\n",
    "4) sauvegarder les donn√©es brutes en CSV,\n",
    "5) poser les bases du nettoyage (√† faire en √©quipe).\n",
    "\n",
    "> **Pourquoi ce format ?**  \n",
    "> Un notebook est id√©al pour apprendre : on alterne **explications** (Markdown) et **code** (Python), et on voit les r√©sultats imm√©diatement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Dossiers de sortie\n",
    "RAW_DIR = Path(\"../data/raw\")\n",
    "CLEAN_DIR = Path(\"../data/cleaned\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "BASE_URL = \"https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/accidents-corporels-de-la-circulation-millesime/records\"\n",
    "\n",
    "SELECT_PARTS = [\n",
    "    \"num_acc\",\n",
    "    \"datetime\",\n",
    "    \"an\",\n",
    "    \"mois\",\n",
    "    \"jour\",\n",
    "    \"hrmn\",\n",
    "    \"lum\",\n",
    "    \"agg\",\n",
    "    '\"int\" as intersection',   # <= r√©serv√© ‚Üí cit√© + alias\n",
    "    \"atm\",\n",
    "    \"col\",\n",
    "    \"dep\",\n",
    "    \"com\",\n",
    "    \"insee\",\n",
    "    \"adr\",\n",
    "    \"lat\",\n",
    "    '\"long\" as lon',           # <= r√©serv√© ‚Üí cit√© + alias\n",
    "    \"surf\",\n",
    "    \"circ\",\n",
    "    \"nbv\",\n",
    "    \"catr\",\n",
    "    \"plan\",\n",
    "    \"prof\",\n",
    "    \"infra\",\n",
    "    \"situ\",\n",
    "    \"gps\",\n",
    "    \"year_georef\",\n",
    "    \"dep_name\",\n",
    "    \"reg_name\",\n",
    "    \"epci_name\"\n",
    "]\n",
    "SELECT = \", \".join(SELECT_PARTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(offset=0, limit=1000):\n",
    "    params = {\n",
    "        \"select\": SELECT,\n",
    "        \"limit\": limit,\n",
    "        \"offset\": offset,\n",
    "        \"order_by\": \"datetime\"   # champ 'safe' pour trier\n",
    "    }\n",
    "    r = requests.get(BASE_URL, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"results\", [])\n",
    "\n",
    "# Test rapide\n",
    "sample = fetch_page(0, 10)\n",
    "df = pd.DataFrame(sample)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 1000\n",
    "PAGE = 100\n",
    "DELAY = 0.3\n",
    "\n",
    "all_rows = []\n",
    "offset = 0\n",
    "while offset < TARGET:\n",
    "    chunk = fetch_page(offset, PAGE)\n",
    "    if not chunk:\n",
    "        break\n",
    "    all_rows.extend(chunk)\n",
    "    offset += PAGE\n",
    "    time.sleep(DELAY)\n",
    "\n",
    "# Sauvegarder un export \"brut\" pour tra√ßabilit√©\n",
    "df_raw = pd.DataFrame(all_rows)\n",
    "df_raw.to_csv(RAW_DIR / \"accidents_sample_raw.csv\", index=False)\n",
    "print(f\"{len(df_raw)} lignes enregistr√©es\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Chargement des donn√©es sources\n",
    "\n",
    "Jusqu‚Äôici, nous avons vu comment interagir avec l‚ÄôAPI publique d‚ÄôOpendatasoft pour r√©cup√©rer les donn√©es dont nous avons besoin.  \n",
    "Un fichier d‚Äôexemple de 1 000 lignes a permis d‚Äôillustrer le principe de pagination et de test de l‚ÄôAPI.\n",
    "\n",
    "Cependant, l‚ÄôAPI limite les extractions √† des paquets de 100 enregistrements, et le jeu complet (plus de 500 000 lignes) aurait demand√© un temps de traitement trop important.  \n",
    "\n",
    "Pour la suite du brief, nous utiliserons donc directement le fichier CSV complet, d√©j√† t√©l√©charg√© et pr√©par√© √† partir de la source officielle :  \n",
    "üëâ [Accidents corporels de la circulation mill√©sim√© ‚Äî Opendatasoft](https://public.opendatasoft.com/explore/assets/accidents-corporels-de-la-circulation-millesime/export/)\n",
    "\n",
    "Ce fichier servira de base √† toutes les √©tapes suivantes de notre pipeline (nettoyage, transformation et analyse)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# # Nettoyage du jeu de donn√©es brut\n",
    "# Ce bloc charge le fichier CSV brut t√©l√©charg√© depuis Opendatasoft,\n",
    "# garde uniquement les colonnes utiles √† notre mod√®le, puis √©crit un CSV nettoy√© dans `data/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Lire (essaie UTF-8 puis fallback latin1 si besoin)\n",
    "try:\n",
    "    df = pd.read_csv(\"../data/raw/accidents-corporels-de-la-circulation-millesime.csv\",\n",
    "                     sep=\";\", dtype=str, encoding=\"utf-8-sig\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(\"../data/raw/accidents-corporels-de-la-circulation-millesime.csv\",\n",
    "                     sep=\";\", dtype=str, encoding=\"latin1\")\n",
    "\n",
    "print(\"Colonnes d√©tect√©es:\", list(df.columns)[:8], \"...\")\n",
    "\n",
    "# 2) Garder seulement les colonnes utiles (sans renommage)\n",
    "cols = [\n",
    "    \"Identifiant de l'accident\",\"Date et heure\",\"Commune\",\"Ann√©e\",\"Mois\",\"Jour\",\"Heure minute\",\n",
    "    \"Lumi√®re\",\"Localisation\",\"Intersection\",\"Conditions atmosph√©riques\",\"Collision\",\n",
    "    \"D√©partement\",\"Code commune\",\"Code Insee\",\"Adresse\",\"Latitude\",\"Longitude\",\n",
    "    \"Surface\",\"Circulation\",\"Nombre de voies\",\"Cat√©gorie route\",\"Plan\",\"Profil\",\n",
    "    \"Infrastructure\",\"Situation\",\"Gps\",\"year_georef\",\n",
    "    \"Nom Officiel D√©partement\",\"Nom Officiel R√©gion\",\"Nom Officiel EPCI\",\"Nom Officiel Commune\",\n",
    "]\n",
    "df_filtered = df[cols]\n",
    "\n",
    "# 3) √âcrire pour Excel ‚Üí UTF-8 avec BOM + pas d‚Äôindex\n",
    "out_path = \"../data/raw/accidents-corporels-de-la-circulation-millesime_raw.csv\"\n",
    "df_filtered.to_csv(out_path, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ CSV cr√©√© (compatible Excel) :\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Nettoyage de base\n",
    "\n",
    "Nous allons maintenant proc√©der √† un premier nettoyage des donn√©es.  \n",
    "L‚Äôobjectif ici est d‚Äôobtenir un fichier exploitable et coh√©rent avant d‚Äôentamer les transformations plus avanc√©es.\n",
    "\n",
    "Ce nettoyage de base consiste √† :\n",
    "- supprimer les doublons √©ventuels,\n",
    "- √©liminer les lignes sans identifiant d‚Äôaccident,\n",
    "- pr√©parer un fichier CSV propre dans le dossier `data/cleaned`.\n",
    "\n",
    "Cette √©tape garantit que les traitements suivants (analyse, enrichissement, agr√©gations) reposeront sur un jeu de donn√©es unique et fiable.\n",
    "\n",
    "\n",
    "A PAUFINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Charger le fichier d√©j√† filtr√©\n",
    "df_raw = pd.read_csv(\n",
    "    \"../data/raw/accidents-corporels-de-la-circulation-millesime_raw.csv\",\n",
    "    sep=\";\",\n",
    "    dtype=str,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Fichier charg√© :\", len(df_raw), \"lignes\")\n",
    "print(\"Colonnes :\", list(df_raw.columns)[:8], \"...\")\n",
    "\n",
    "# R√®gles minimales : garder une cl√©, d√©doublonner\n",
    "df_clean = df_raw.copy()\n",
    "\n",
    "# supprimer les lignes sans identifiant d'accident\n",
    "# le nom exact de la colonne est \"Identifiant de l'accident\"\n",
    "if \"Identifiant de l'accident\" in df_clean.columns:\n",
    "    df_clean = (\n",
    "        df_clean.dropna(subset=[\"Identifiant de l'accident\"])\n",
    "                .drop_duplicates(subset=[\"Identifiant de l'accident\"])\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Apr√®s nettoyage :\", len(df_clean), \"lignes\")\n",
    "\n",
    "# Sauvegarde du jeu nettoy√©\n",
    "CLEAN_DIR = Path(\"../data/cleaned\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "clean_csv = CLEAN_DIR / \"accidents_clean.csv\"\n",
    "df_clean.to_csv(clean_csv, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"üíæ Fichier nettoy√© enregistr√© :\", clean_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Prochaines √©tapes \n",
    "\n",
    "- G√©n√©rer `dim_time` √† partir des dates (ou d‚Äôun calendrier)  \n",
    "- Cr√©er `dim_location` (distinct de `codeinsee`, `departement`, lat/lon si dispo)  \n",
    "- Cr√©er `dim_conditions` (distinct de `lumiere`, `meteo`, `type_route`, `type_de_collision`)  \n",
    "- Mapper les IDs (cl√© naturelle ‚Üí cl√© de substitution)  \n",
    "- Remplir `fact_accident` avec les mesures (`ttue`, `tbg`, `tbl`, `tindm`, `grav`)  \n",
    "- √âcrire `sql/schema.sql` et (option) charger via SQLAlchemy dans SQLite/MySQL/PostgreSQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
